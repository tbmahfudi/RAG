# RAG FastAPI Application

A complete RAG (Retrieval-Augmented Generation) system with FastAPI backend and vanilla JavaScript frontend. Upload documents and ask questions using AI-powered chat with source citations.

## ğŸŒŸ Features

### Backend
- ğŸ“„ **Multiple Document Upload**: Batch upload PDF and TXT files
- ğŸ” **Vector Search**: ChromaDB for efficient similarity search
- ğŸ¤– **OpenAI Integration**: GPT-4o-mini for intelligent responses
- ğŸ’¬ **Streaming Responses**: Real-time SSE (Server-Sent Events)
- ğŸ“Š **Source Citations**: Responses include relevant document excerpts
- âš™ï¸ **Configurable LLM**: Easy to switch between OpenAI models

### Frontend
- ğŸ¨ **Modern UI**: Beautiful interface with Tailwind CSS
- ğŸ“¤ **Drag & Drop**: Upload multiple files at once
- âš¡ **Real-time Updates**: Streaming chat responses
- ğŸ“± **Responsive Design**: Works on desktop and mobile
- ğŸ¯ **No Build Required**: Pure JavaScript, no frameworks

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose (optional)
- OpenAI API Key

### Option 1: Docker Compose (Recommended)

1. **Clone and setup**
   ```bash
   git clone <repository-url>
   cd RAG
   ```

2. **Configure environment**
   ```bash
   cp backend/.env.example backend/.env
   ```
   Edit `backend/.env` and add your OpenAI API key:
   ```
   OPENAI_API_KEY=sk-your-actual-api-key-here
   ```

3. **Start services**
   ```bash
   docker-compose up -d
   ```

4. **Access the application**
   - Frontend: http://localhost:8081
   - Backend API: http://localhost:8001
   - API Docs: http://localhost:8001/docs

### Option 2: Local Development

#### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your OpenAI API key

# Run server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8001
```

#### Frontend Setup

```bash
cd frontend

# Serve with Python
python -m http.server 8081

# Or with Node.js
npx http-server -p 8081
```

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API routes
â”‚   â”‚   â”‚   â””â”€â”€ routes.py     # Endpoints
â”‚   â”‚   â”œâ”€â”€ models/           # Pydantic models
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py    # Data schemas
â”‚   â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_service.py      # ChromaDB operations
â”‚   â”‚   â”‚   â”œâ”€â”€ document_service.py    # Document processing
â”‚   â”‚   â”‚   â””â”€â”€ chat_service.py        # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ utils/            # Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ file_parser.py         # PDF/TXT parsing
â”‚   â”‚   â”‚   â””â”€â”€ text_splitter.py       # Text chunking
â”‚   â”‚   â”œâ”€â”€ config.py         # Configuration
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI app
â”‚   â”œâ”€â”€ data/                 # Local storage (gitignored)
â”‚   â”‚   â”œâ”€â”€ uploads/          # Uploaded files
â”‚   â”‚   â””â”€â”€ chromadb/         # Vector database
â”‚   â”œâ”€â”€ tests/                # Unit tests
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ api.js            # API client
â”‚   â”‚   â”œâ”€â”€ upload.js         # Upload handling
â”‚   â”‚   â”œâ”€â”€ chat.js           # Chat with streaming
â”‚   â”‚   â””â”€â”€ app.js            # Initialization
â”‚   â”œâ”€â”€ index.html            # Main page
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docker-compose.yml        # Docker orchestration
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the root directory:

```bash
# Required
OPENAI_API_KEY=sk-your-api-key-here

# Optional (with defaults)
LLM_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_TOP_K=5
MAX_FILE_SIZE_MB=10
```

### Backend Configuration

All settings in `backend/app/config.py`:

| Setting | Default | Description |
|---------|---------|-------------|
| `OPENAI_API_KEY` | Required | OpenAI API key |
| `LLM_MODEL` | `gpt-4o-mini` | OpenAI model for chat |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | Model for embeddings |
| `CHUNK_SIZE` | `1000` | Text chunk size (characters) |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `RETRIEVAL_TOP_K` | `5` | Number of chunks to retrieve |
| `MAX_FILE_SIZE_MB` | `10` | Maximum upload file size |

## ğŸ“– API Documentation

### Endpoints

#### Upload Documents
```http
POST /api/documents/upload
Content-Type: multipart/form-data

Body: files[] (multiple files)
```

#### List Documents
```http
GET /api/documents
```

#### Chat (Non-Streaming)
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Your question",
  "conversation_id": null,
  "top_k": 5,
  "temperature": 0.7
}
```

#### Chat (Streaming)
```http
GET /api/chat/stream?message=Your%20question&top_k=5&temperature=0.7
```

Full API documentation available at: http://localhost:8001/docs

## ğŸ—ï¸ Architecture

### RAG Pipeline

```
User Question
    â†“
[1] Query Embedding (OpenAI)
    â†“
[2] Vector Search (ChromaDB)
    â†“
[3] Retrieve Top K Chunks
    â†“
[4] Build Context Prompt
    â†“
[5] LLM Generation (OpenAI)
    â†“
[6] Stream Response (SSE)
    â†“
Answer + Sources
```

### Document Processing Pipeline

```
Upload File(s)
    â†“
[1] Validate Type & Size
    â†“
[2] Extract Text (PyPDF2)
    â†“
[3] Split into Chunks
    â†“
[4] Generate Embeddings (OpenAI)
    â†“
[5] Store in ChromaDB
    â†“
Success Response
```

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend
pytest tests/
```

### API Testing with curl

**Upload a document:**
```bash
curl -X POST http://localhost:8001/api/documents/upload \
  -F "files=@document.pdf"
```

**Chat:**
```bash
curl -X POST http://localhost:8001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is the main topic?"}'
```

**Streaming chat:**
```bash
curl -N "http://localhost:8001/api/chat/stream?message=Hello"
```

## ğŸ”’ Security Considerations

- **API Keys**: Never commit `.env` files to version control
- **File Upload**: Validates file types and sizes
- **CORS**: Configure for production in `backend/app/main.py`
- **Rate Limiting**: Consider adding rate limits for production
- **Authentication**: Add JWT auth for multi-user deployments

## ğŸš¢ Production Deployment

### Using Docker Compose

```bash
# Production build
docker-compose -f docker-compose.yml up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Environment Setup

For production, update `docker-compose.yml`:
- Remove volume mount for hot reload
- Set `DEBUG=false`
- Configure proper CORS origins
- Add reverse proxy (nginx/traefik)
- Enable HTTPS

## ğŸ“Š Monitoring

### Health Check
```bash
curl http://localhost:8001/health
```

### Logs
```bash
# Docker
docker-compose logs -f backend

# Local
tail -f backend/app.log
```

## ğŸ› Troubleshooting

### Common Issues

**1. OpenAI API errors**
- Check API key is correct in `.env`
- Verify you have credits in your OpenAI account
- Check rate limits

**2. Upload fails**
- Ensure file is PDF or TXT
- Check file size < 10MB
- Verify backend is running

**3. ChromaDB errors**
- Clear `backend/data/chromadb/` and restart
- Ensure write permissions on data directory

**4. CORS errors**
- Update `allow_origins` in `backend/app/main.py`
- Check frontend is served from allowed origin

## ğŸ›£ï¸ Roadmap

### Planned Features

- [ ] Document deletion
- [ ] Multi-user authentication
- [ ] Conversation history persistence
- [ ] Support for more file types (DOCX, MD, HTML)
- [ ] Hybrid search (vector + keyword)
- [ ] Re-ranking for better retrieval
- [ ] Multiple LLM providers (Anthropic, Gemini)
- [ ] Admin dashboard
- [ ] Usage analytics

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **FastAPI**: Modern Python web framework
- **OpenAI**: LLM and embeddings
- **ChromaDB**: Vector database
- **Tailwind CSS**: UI styling

## ğŸ“ Support

For issues and questions:
- Create an issue on GitHub
- Check existing documentation in `backend/README.md` and `frontend/README.md`

---

**Built with â¤ï¸ using FastAPI, OpenAI, and vanilla JavaScript**
